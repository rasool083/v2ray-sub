import requests
import re
import base64
import json
import urllib.parse
from bs4 import BeautifulSoup
import logging
import time

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ù‡ØªØ± Ø¯Ø± GitHub Actions ---
logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ---
TELEGRAM_CHANNEL_URL = "https://t.me/s/ConfigsHubPlus"
MAX_CONFIGS = 400
NEW_CONFIG_NAME = "t.me/rghoddoosi Ø±Ø³ÙˆÙ„ Ù‚Ø¯ÙˆØ³ÛŒ"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9'
}
REQUEST_TIMEOUT = 30

def is_config_valid(config_str: str) -> bool:
    """ÛŒÚ© Ø±Ø´ØªÙ‡ Ú©Ø§Ù†ÙÛŒÚ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ù† Ø³Ø§Ø®ØªØ§Ø± Ø§ÙˆÙ„ÛŒÙ‡ Ø¢Ù† Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    if not config_str: return False
    try:
        if config_str.startswith("vless://") or config_str.startswith("trojan://"):
            uuid_pattern = r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}'
            return '@' in config_str and re.search(uuid_pattern, config_str) is not None
        elif config_str.startswith("vmess://"):
            b64_part = config_str[8:]
            padded_b64 = b64_part + '=' * (-len(b64_part) % 4)
            data = json.loads(base64.b64decode(padded_b64).decode('utf-8'))
            return all(key in data for key in ['add', 'port', 'id', 'ps'])
        elif config_str.startswith("ss://"):
            return '@' in config_str and '#' in config_str
    except Exception:
        return False
    return False

def extract_flag_from_name(name: str) -> str:
    """Ù¾Ø±Ú†Ù… Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ú©Ø´ÙˆØ± Ø±Ø§ Ø§Ø² Ù†Ø§Ù… Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    flag_pattern = r'(\[[A-Z]{2}\])|([ğŸ‡¦-ğŸ‡¿]{2})'
    match = re.search(flag_pattern, name, re.IGNORECASE)
    if match:
        return next((group for group in match.groups() if group is not None), "ğŸ‡®ğŸ‡·")
    return "ğŸ‡®ğŸ‡·"

def process_config(config_str: str) -> str | None:
    """ÛŒÚ© Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø¹ØªØ¨Ø± Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø±Ø¯Ù‡ Ùˆ Ù†Ø§Ù… Ø¢Ù† Ø±Ø§ ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."""
    try:
        if config_str.startswith("vmess://"):
            b64_part = config_str[8:]
            padded_b64 = b64_part + '=' * (-len(b64_part) % 4)
            config_data = json.loads(base64.b64decode(padded_b64).decode('utf-8'))
            flag = extract_flag_from_name(config_data.get('ps', ''))
            config_data['ps'] = f"{flag} {NEW_CONFIG_NAME}"
            new_json = json.dumps(config_data, ensure_ascii=False, separators=(',', ':'))
            return "vmess://" + base64.b64encode(new_json.encode('utf-8')).decode('utf-8')
        else:
            parts = config_str.split("#", 1)
            base_config = parts[0]
            old_name = urllib.parse.unquote(parts[1]) if len(parts) > 1 else ""
            flag = extract_flag_from_name(old_name)
            new_name_encoded = urllib.parse.quote(f"{flag} {NEW_CONFIG_NAME}")
            return f"{base_config}#{new_name_encoded}"
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù†ÙÛŒÚ¯: {config_str[:30]}... | Ø®Ø·Ø§: {e}")
        return None

def scrape_telegram_channel():
    """
    ØµÙØ­Ø§Øª Ú©Ø§Ù†Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù… Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Ù¾ÛŒÙ…Ø§ÛŒØ´ Ù…Ù‚Ø§ÙˆÙ…ØŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ù¾ÛŒÙ…Ø§ÛŒØ´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    all_configs = []
    base_url = "https://t.me"
    current_path = f"/s/{TELEGRAM_CHANNEL_URL.split('/')[-1]}"
    
    # Ø­Ù„Ù‚Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒÙ…Ø§ÛŒØ´ ØµÙØ­Ø§Øª ØªØ§ Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ Ø³Ù‚Ù Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§
    for page_num in range(1, 31): # Ø­Ø¯Ø§Ú©Ø«Ø± 30 ØµÙØ­Ù‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        if len(all_configs) >= MAX_CONFIGS:
            logging.info("Ø¨Ù‡ Ø³Ù‚Ù Û´Û°Û° Ú©Ø§Ù†ÙÛŒÚ¯ Ø±Ø³ÛŒØ¯ÛŒÙ…. Ù¾ÛŒÙ…Ø§ÛŒØ´ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            break

        full_url = base_url + current_path
        logging.info(f"--- Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ {page_num}: {full_url} ---")

        try:
            response = requests.get(full_url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø§Ø² ØªÚ¯â€ŒÙ‡Ø§ÛŒ <code>
            code_blocks = soup.find_all("code")
            found_in_page = 0
            for block in code_blocks:
                for line in block.get_text(strip=True).splitlines():
                    clean_line = line.strip()
                    if is_config_valid(clean_line):
                        processed = process_config(clean_line)
                        if processed:
                            all_configs.append(processed)
                            found_in_page += 1
            
            logging.info(f"ØªØ¹Ø¯Ø§Ø¯ {found_in_page} Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø¹ØªØ¨Ø± Ø¯Ø± Ø§ÛŒÙ† ØµÙØ­Ù‡ ÛŒØ§ÙØª Ø´Ø¯.")

            # --- Ø¨Ø®Ø´ Ú©Ù„ÛŒØ¯ÛŒ: Ù…Ù†Ø·Ù‚ Ø¬Ø¯ÛŒØ¯ Ùˆ Ù…Ù‚Ø§ÙˆÙ… Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ ---
            # Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ ØªÚ¯ a Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÛŒÙ… Ú©Ù‡ href Ø¢Ù† Ø¨Ø§ ?before= Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            older_posts_link = soup.select_one('a.tgme_widget_message_more[href^="?before="]')
            
            if older_posts_link:
                current_path = older_posts_link['href']
                # Ø§ÙØ²ÙˆØ¯Ù† ÛŒÚ© ØªØ§Ø®ÛŒØ± Ú©ÙˆÚ†Ú© Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†
                time.sleep(1) 
            else:
                logging.info("Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ Ú©Ø§Ù†Ø§Ù„ Ø±Ø³ÛŒØ¯ÛŒÙ… ÛŒØ§ Ù„ÛŒÙ†Ú© ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù¾Ø§ÛŒØ§Ù† Ù¾ÛŒÙ…Ø§ÛŒØ´.")
                break

        except requests.exceptions.RequestException as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…: {e}")
            break
    
    unique_configs = list(dict.fromkeys(all_configs))
    return unique_configs[:MAX_CONFIGS]

def main():
    logging.info("--- Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ V2Ray (Ù†Ø³Ø®Ù‡ Ø¨Ø§ Ù¾ÛŒÙ…Ø§ÛŒØ´ Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡) ---")
    try:
        final_configs = scrape_telegram_channel()
        if not final_configs:
            logging.warning("Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø¹ØªØ¨Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            content = "âš ï¸ Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù„Ø­Ø¸Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯."
        else:
            logging.info(f"Ù…Ø¬Ù…ÙˆØ¹Ø§Ù‹ {len(final_configs)} Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ùˆ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÛŒØ§ÙØª Ø´Ø¯.")
            content = "\n".join(final_configs)
        encoded_content = base64.b64encode(content.encode('utf-8')).decode('utf-8')
    except Exception as e:
        logging.error(f"ÛŒÚ© Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ†Ø´Ø¯Ù‡ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø±Ø® Ø¯Ø§Ø¯: {e}")
        error_message = f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª: {e}"
        encoded_content = base64.b64encode(error_message.encode('utf-8')).decode('utf-8')

    with open("sub.txt", "w", encoding="utf-8") as f:
        f.write(encoded_content)
    logging.info(f"--- Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. ÙØ§ÛŒÙ„ sub.txt Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯. ---")

if __name__ == "__main__":
    main()
